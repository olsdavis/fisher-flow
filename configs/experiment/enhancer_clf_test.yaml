# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: enhancer
  - override /model: enhancer_mel_clf
  - override /callbacks: default
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["enhancer", "cnn", "clf"]

seed: 12345

trainer:
  max_steps: 450000
  max_epochs: 100000
  limit_train_batches: null
  gradient_clip_val: 1.0
  check_val_every_n_epoch: 1
  accelerator: 'gpu'
  devices: [3]


model:
  model: 
    hidden: 128
    depth: 1
    mode: "dirichlet"
    num_cls: 81 # output classes 47 for MEL
    #num_cls: 47 # output classes 47 for MEL
    clean_data: true
    classifier: true
    dropout: 0.0
  num_cls: 81

data:
  batch_size: 256
  dataset: "FlyBrain" # choices are "MEL2" and "FlyBrain" 47 classes and 81 classes

model_checkpoint:
  monitor: 'val/loss' # val_perplexity

logger:
  wandb:
    project: sfm
    tags: ${tags}
    group: enhancer
    name: test_enhancer_clf_stark_${data.dataset}_s${seed}
  aim:
    experiment: "enhancer"

# this our own custom run
#ckpt_path: xxx

# model from Stark et al.
ckpt_path: workdir/clsDNAclean_cnn_1stack_2023-12-30_15-01-30/epoch=15-step=10480.ckpt