{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolant\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_map(x_0, x_1):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        The logarithmic map at x_0 towards x_1.\n",
    "    \"\"\"\n",
    "    x_0rt = x_0.sqrt()\n",
    "    x_1rt = x_1.sqrt()\n",
    "    dotrt = (x_0rt * x_1rt).sum(dim=-1, keepdim=True)\n",
    "    dist = 2.0 * torch.arccos(dotrt)\n",
    "    denom = torch.sqrt(1.0 - dotrt)\n",
    "    fact = x_0rt * x_1rt - dotrt * x_0\n",
    "    return (dist / denom) * fact\n",
    "\n",
    "\n",
    "def exp_map(p, v):\n",
    "    vp = v / (p.sqrt() + 1e-6)\n",
    "    vpn = vp.norm(dim=-1, keepdim=True)\n",
    "    cst = 0.5 * (p + vp.square() / (vpn ** 2))\n",
    "    cos_term = 0.5 * (p - vp.square() / (vpn ** 2)) * torch.cos(vpn)\n",
    "    sin_term = (vp / vpn) * p.sqrt() * torch.sin(vpn)\n",
    "    return cst + cos_term + sin_term\n",
    "\n",
    "\n",
    "def interpolant(x_0, x_1, t):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - x_0, x_1: Tensor of dimensions [B, D] where each point of the batch\n",
    "        lies on the D-simplex;\n",
    "        - t: the time Tensor of shape [B, 1].\n",
    "    Returns:\n",
    "        The geodesic interpolant.\n",
    "    \"\"\"\n",
    "    \"\"\"v = t * log_map(x_0, x_1)\n",
    "    vp = v / x_0.sqrt()\n",
    "    vpn = torch.norm(vp, dim=-1, keepdim=True)\n",
    "\n",
    "    cst = 0.5 * (x_0 + vp.square() / (vpn ** 2))\n",
    "    cos_term = 0.5 * (x_0 - vp.square() / (vpn ** 2)) * torch.cos(vpn)\n",
    "    sin_term = (vp / vpn) * x_0.sqrt() * torch.sin(vpn)\"\"\"\n",
    "    return exp_map(x_0, t * log_map(x_0, x_1))\n",
    "\n",
    "\n",
    "def interpolant_derivative(x_0, x_1, t):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - x_0, x_1: Tensor of dimensions [B, D] where each point of the batch\n",
    "        lies on the D-simplex;\n",
    "        - t: the time Tensor of shape [B, 1].\n",
    "    Returns:\n",
    "        The derivative of the interpolant.\n",
    "    \"\"\"\n",
    "    v = t * log_map(x_0, x_1)\n",
    "    vp = v / torch.sqrt(x_0)\n",
    "    vpn = torch.norm(vp, dim=-1, keepdim=True)\n",
    "    sin_term = -0.5 * (vpn * x_0 - vp.square() / vpn) * torch.sin(vpn * t)\n",
    "    cos_term = vp * torch.sqrt(x_0) * torch.sin(vpn * t)\n",
    "    return (sin_term + cos_term) * log_map(x_0, x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, size: int, scale: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x * self.scale\n",
    "        half_size = self.size // 2\n",
    "        emb = torch.log(torch.Tensor([10000.0]).to(x.device)) / (half_size - 1)\n",
    "        emb = torch.exp(-emb * torch.arange(half_size).to(x.device))\n",
    "        emb = x.unsqueeze(-1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n",
    "        return emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class LinearEmbedding(nn.Module):\n",
    "    def __init__(self, size: int, scale: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x / self.size * self.scale\n",
    "        return x.unsqueeze(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class LearnableEmbedding(nn.Module):\n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.linear = nn.Linear(1, size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.linear(x.unsqueeze(-1).float() / self.size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class IdentityEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x.unsqueeze(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class ZeroEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x.unsqueeze(-1) * 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, size: int, type: str, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        if type == \"sinusoidal\":\n",
    "            self.layer = SinusoidalEmbedding(size, **kwargs)\n",
    "        elif type == \"linear\":\n",
    "            self.layer = LinearEmbedding(size, **kwargs)\n",
    "        elif type == \"learnable\":\n",
    "            self.layer = LearnableEmbedding(size)\n",
    "        elif type == \"zero\":\n",
    "            self.layer = ZeroEmbedding()\n",
    "        elif type == \"identity\":\n",
    "            self.layer = IdentityEmbedding()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown positional embedding type: {type}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self, size: int, t_emb_size: int = 0, add_t_emb=False, concat_t_emb=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        in_size = size + t_emb_size if concat_t_emb else size\n",
    "        self.ff = nn.Linear(in_size, size)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "        self.add_t_emb = add_t_emb\n",
    "        self.concat_t_emb = concat_t_emb\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_emb: torch.Tensor):\n",
    "        in_arg = torch.cat([x, t_emb], dim=-1) if self.concat_t_emb else x\n",
    "        out = x + self.act(self.ff(in_arg))\n",
    "\n",
    "        if self.add_t_emb:\n",
    "            out = out + t_emb\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 128,\n",
    "        hidden_layers: int = 3,\n",
    "        emb_size: int = 128,\n",
    "        out_dim: int = 2,\n",
    "        time_emb: str = \"sinusoidal\",\n",
    "        input_emb: str = \"sinusoidal\",\n",
    "        add_t_emb: bool = False,\n",
    "        concat_t_emb: bool = False,\n",
    "        input_dim: int = 2,\n",
    "        energy_function=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add_t_emb = add_t_emb\n",
    "        self.concat_t_emb = concat_t_emb\n",
    "\n",
    "        self.time_mlp = PositionalEmbedding(emb_size, time_emb)\n",
    "\n",
    "        positional_embeddings = []\n",
    "        for i in range(input_dim):\n",
    "            embedding = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n",
    "\n",
    "            self.add_module(f\"input_mlp{i}\", embedding)\n",
    "\n",
    "            positional_embeddings.append(embedding)\n",
    "\n",
    "        self.channels = 1\n",
    "        self.self_condition = False\n",
    "        concat_size = len(self.time_mlp.layer) + sum(\n",
    "            map(lambda x: len(x.layer), positional_embeddings)\n",
    "        )\n",
    "\n",
    "        layers = [nn.Linear(concat_size, hidden_size)]\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(Block(hidden_size, emb_size, add_t_emb, concat_t_emb))\n",
    "\n",
    "        in_size = emb_size + hidden_size if concat_t_emb else emb_size\n",
    "        layers.append(nn.Linear(in_size, out_dim))\n",
    "\n",
    "        self.layers = layers\n",
    "        self.joint_mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, t, x_self_cond=False):\n",
    "        positional_embs = [\n",
    "            self.get_submodule(f\"input_mlp{i}\")(x[:, i]) for i in range(x.shape[-1])\n",
    "        ]\n",
    "\n",
    "        t_emb = self.time_mlp(t.squeeze())\n",
    "        x = torch.cat((*positional_embs, t_emb), dim=-1)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                x = nn.GELU()(layer(x))\n",
    "                if self.add_t_emb:\n",
    "                    x = x + t_emb\n",
    "\n",
    "            elif i == len(self.layers) - 1:\n",
    "                if self.concat_t_emb:\n",
    "                    x = torch.cat([x, t_emb], dim=-1)\n",
    "\n",
    "                x = layer(x)\n",
    "\n",
    "            else:\n",
    "                x = layer(x, t_emb)\n",
    "\n",
    "        #return torch.softmax(x, dim=-1)\n",
    "        x_3 = -x[:, 0] - x[:, 1]\n",
    "        return torch.cat([x, x_3.unsqueeze(-1)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int, depth: int, hidden: int, batch_norm: bool = False, time_fts: int = 0):\n",
    "        super().__init__()\n",
    "        self.time_fts = time_fts\n",
    "        net = []\n",
    "        for i in range(depth):\n",
    "            # + 1 for time\n",
    "            out = hidden if i < depth - 1 else dim\n",
    "            net += [nn.Linear(\n",
    "                dim + 1 + time_fts if i == 0 else hidden,\n",
    "                out\n",
    "            )]\n",
    "            if i < depth - 1:\n",
    "                if batch_norm:\n",
    "                    net += [nn.BatchNorm1d(out)]\n",
    "                net += [nn.ReLU()]\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = torch.cat([x, t] + [torch.cos(t ** i) for i in range(self.time_fts)], dim=-1)\n",
    "        return torch.softmax(self.net(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dirichlet_3d(points):\n",
    "    v_a = torch.Tensor([[0, 1.0]])\n",
    "    v_b = torch.Tensor([[-0.5, 0]])\n",
    "    v_c = torch.Tensor([[0.5, 0]])\n",
    "    points = points[:, 0].unsqueeze(-1) * v_a + points[:, 1].unsqueeze(-1) * v_b + points[:, 2].unsqueeze(-1) * v_c\n",
    "    plt.scatter(points[:, 0], points[:, 1])\n",
    "    plt.show()\n",
    "\n",
    "def plot(points, points_b):\n",
    "\n",
    "    v_a = torch.Tensor([[0, 1.0]])\n",
    "    v_b = torch.Tensor([[-0.5, 0]])\n",
    "    v_c = torch.Tensor([[0.5, 0]])\n",
    "    points = points[:, 0].unsqueeze(-1) * v_a + points[:, 1].unsqueeze(-1) * v_b + points[:, 2].unsqueeze(-1) * v_c\n",
    "    plt.scatter(points[:, 0], points[:, 1])\n",
    "\n",
    "\n",
    "    v_a = torch.Tensor([[0, 1.0]])\n",
    "    v_b = torch.Tensor([[-0.5, 0]])\n",
    "    v_c = torch.Tensor([[0.5, 0]])\n",
    "    points_b = points_b[:, 0].unsqueeze(-1) * v_a + points_b[:, 1].unsqueeze(-1) * v_b + points_b[:, 2].unsqueeze(-1) * v_c\n",
    "    plt.scatter(points_b[:, 0], points_b[:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_dirichlet(points: int, alpha):\n",
    "    ret = []\n",
    "    for _ in range(points):\n",
    "        p = np.random.dirichlet(alpha)\n",
    "        ret += [torch.Tensor(p)]\n",
    "    return torch.stack(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirichlet_3d(generate_simple_dirichlet(1000, [1, 10, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = generate_simple_dirichlet(10000, [1, 10, 1])\n",
    "dataset = TensorDataset(raw_dataset)\n",
    "test_dataset = TensorDataset(generate_simple_dirichlet(1000, [1, 10, 1]))\n",
    "train_loader = DataLoader(dataset, 128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, 128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, epochs: int, lr: float = 1e-3, time_eps: float = 0.0):\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    per_epoch = []\n",
    "    per_epoch_test = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for x_1 in train_loader:\n",
    "            x_1 = x_1[0]\n",
    "            optimizer.zero_grad()\n",
    "            times = torch.rand((x_1.size(0), 1)) * (1.0 - time_eps) + time_eps\n",
    "\n",
    "            # Mapping uniform Dirichlet to our target distribution\n",
    "            x_0 = torch.stack([torch.Tensor(np.random.dirichlet([1, 1, 1])) for _ in range(x_1.size(0))])\n",
    "            x_t = interpolant(x_0, x_1, times)\n",
    "            target = log_map(x_0, x_1)\n",
    "            # print(x_t.std(), x_t.mean(), target.std(), target.mean())\n",
    "            loss = criterion(model(x_t, times), target)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += [loss.item()]\n",
    "        print(f\"--- Epoch {epoch+1:03d}/{epochs:03d}: mean loss {np.mean(losses):.5f}\")\n",
    "        per_epoch += [np.mean(losses)]\n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        with torch.no_grad():\n",
    "            for x_1 in test_loader:\n",
    "                x_1 = x_1[0]\n",
    "                times = torch.rand((x_1.size(0), 1)) * (1.0 - time_eps) + time_eps\n",
    "                # Mapping uniform Dirichlet to our target distribution\n",
    "                x_0 = torch.stack([torch.Tensor(np.random.dirichlet([1, 1, 1])) for _ in range(x_1.size(0))])\n",
    "                x_t = interpolant(x_0, x_1, times)\n",
    "                target = log_map(x_0, x_1)\n",
    "                # print(x_t.std(), x_t.mean(), target.std(), target.mean())\n",
    "                loss = criterion(model(x_t, times), target)\n",
    "                test_loss += [loss.item()]\n",
    "        print(f\"Test loss {np.mean(test_loss):.5f}\")\n",
    "        per_epoch_test += [np.mean(test_loss)]\n",
    "    plt.plot(per_epoch, label=\"Train\")\n",
    "    plt.plot(per_epoch_test, label=\"Test\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "# model = train(MLP(3, 4, 64, False), 50)\n",
    "model = train(MyMLP(input_dim=3, out_dim=2), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def euler(model, x_0, steps: int = 1000):\n",
    "    # x_0 batched\n",
    "    delta = 1.0 / steps\n",
    "    x = x_0\n",
    "    for i in range(steps):\n",
    "        t = torch.ones((x_0.size(0), 1)) * delta * i\n",
    "        x = x + delta * model(x, t)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_points(points):\n",
    "    v_a = torch.Tensor([[0, 1.0]])\n",
    "    v_b = torch.Tensor([[-0.5, 0]])\n",
    "    v_c = torch.Tensor([[0.5, 0]])\n",
    "    points = points[:, 0].unsqueeze(-1) * v_a + points[:, 1].unsqueeze(-1) * v_b + points[:, 2].unsqueeze(-1) * v_c\n",
    "    return points\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def tangent_euler(model, x_0, steps: int = 1000):\n",
    "    dt = 1.0 / steps\n",
    "    x = x_0\n",
    "    every = 100\n",
    "    xs = []\n",
    "    for i in range(steps):\n",
    "        t = torch.ones((x.size(0), 1)) * dt * (i + 1)\n",
    "        x = exp_map(x, model(x, t) * dt)\n",
    "        if (i + 1) % every == 0:\n",
    "            xs += [x]\n",
    "    f, axs = plt.subplots(nrows=(len(xs) // 4) + 1, ncols=4, figsize=(15, 15))\n",
    "    for i, x in enumerate(xs):\n",
    "        pts = get_points(x)\n",
    "        axs[i // 4, i % 4].scatter(pts[:, 0], pts[:, 1], label=f\"t={(i + 1) * every}\")\n",
    "    plt.show()\n",
    "    return x\n",
    "\n",
    "\n",
    "def viz_model(model):\n",
    "    points = torch.stack([torch.Tensor(np.random.dirichlet([1, 1, 1])) for _ in range(1000)])\n",
    "    dest = tangent_euler(model, points)\n",
    "    plot(raw_dataset, dest)\n",
    "    plot(points, dest)\n",
    "\n",
    "\n",
    "viz_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
