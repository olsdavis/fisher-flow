{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from util import NSimplex, define_style\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "define_style()\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, size: int, scale: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x * self.scale\n",
    "        half_size = self.size // 2\n",
    "        emb = torch.log(torch.Tensor([10000.0]).to(x.device)) / (half_size - 1)\n",
    "        emb = torch.exp(-emb * torch.arange(half_size).to(x.device))\n",
    "        emb = x.unsqueeze(-1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n",
    "        return emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class LinearEmbedding(nn.Module):\n",
    "    def __init__(self, size: int, scale: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x / self.size * self.scale\n",
    "        return x.unsqueeze(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class LearnableEmbedding(nn.Module):\n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.linear = nn.Linear(1, size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.linear(x.unsqueeze(-1).float() / self.size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class IdentityEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x.unsqueeze(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class ZeroEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x.unsqueeze(-1) * 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, size: int, type: str, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        if type == \"sinusoidal\":\n",
    "            self.layer = SinusoidalEmbedding(size, **kwargs)\n",
    "        elif type == \"linear\":\n",
    "            self.layer = LinearEmbedding(size, **kwargs)\n",
    "        elif type == \"learnable\":\n",
    "            self.layer = LearnableEmbedding(size)\n",
    "        elif type == \"zero\":\n",
    "            self.layer = ZeroEmbedding()\n",
    "        elif type == \"identity\":\n",
    "            self.layer = IdentityEmbedding()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown positional embedding type: {type}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self, size: int, t_emb_size: int = 0, add_t_emb=False, concat_t_emb=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        in_size = size + t_emb_size if concat_t_emb else size\n",
    "        self.ff = nn.Linear(in_size, size)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "        self.add_t_emb = add_t_emb\n",
    "        self.concat_t_emb = concat_t_emb\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_emb: torch.Tensor):\n",
    "        in_arg = torch.cat([x, t_emb], dim=-1) if self.concat_t_emb else x\n",
    "        out = x + self.act(self.ff(in_arg))\n",
    "\n",
    "        if self.add_t_emb:\n",
    "            out = out + t_emb\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 128,\n",
    "        hidden_layers: int = 3,\n",
    "        emb_size: int = 128,\n",
    "        out_dim: int = 2,\n",
    "        time_emb: str = \"sinusoidal\",\n",
    "        input_emb: str = \"sinusoidal\",\n",
    "        add_t_emb: bool = False,\n",
    "        concat_t_emb: bool = False,\n",
    "        input_dim: int = 2,\n",
    "        energy_function=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add_t_emb = add_t_emb\n",
    "        self.concat_t_emb = concat_t_emb\n",
    "\n",
    "        self.time_mlp = PositionalEmbedding(emb_size, time_emb)\n",
    "\n",
    "        positional_embeddings = []\n",
    "        for i in range(input_dim):\n",
    "            embedding = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n",
    "\n",
    "            self.add_module(f\"input_mlp{i}\", embedding)\n",
    "\n",
    "            positional_embeddings.append(embedding)\n",
    "\n",
    "        self.channels = 1\n",
    "        self.self_condition = False\n",
    "        concat_size = len(self.time_mlp.layer) + sum(\n",
    "            map(lambda x: len(x.layer), positional_embeddings)\n",
    "        )\n",
    "\n",
    "        layers = [nn.Linear(concat_size, hidden_size)]\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(Block(hidden_size, emb_size, add_t_emb, concat_t_emb))\n",
    "\n",
    "        in_size = emb_size + hidden_size if concat_t_emb else emb_size\n",
    "        layers.append(nn.Linear(in_size, out_dim))\n",
    "\n",
    "        self.layers = layers\n",
    "        self.joint_mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, t, x_self_cond=False):\n",
    "        positional_embs = [\n",
    "            self.get_submodule(f\"input_mlp{i}\")(x[:, i]) for i in range(x.shape[-1])\n",
    "        ]\n",
    "\n",
    "        t_emb = self.time_mlp(t.squeeze())\n",
    "        x = torch.cat((*positional_embs, t_emb), dim=-1)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                x = nn.GELU()(layer(x))\n",
    "                if self.add_t_emb:\n",
    "                    x = x + t_emb\n",
    "\n",
    "            elif i == len(self.layers) - 1:\n",
    "                if self.concat_t_emb:\n",
    "                    x = torch.cat([x, t_emb], dim=-1)\n",
    "\n",
    "                x = layer(x)\n",
    "\n",
    "            else:\n",
    "                x = layer(x, t_emb)\n",
    "\n",
    "        #return torch.softmax(x, dim=-1)\n",
    "        x_3 = -x[:, 0] - x[:, 1]\n",
    "        return torch.cat([x, x_3.unsqueeze(-1)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int, depth: int, hidden: int, batch_norm: bool = False, time_fts: int = 0):\n",
    "        super().__init__()\n",
    "        self.time_fts = time_fts\n",
    "        net = []\n",
    "        for i in range(depth):\n",
    "            # + 1 for time\n",
    "            out = hidden if i < depth - 1 else dim\n",
    "            net += [nn.Linear(\n",
    "                dim + 1 + time_fts if i == 0 else hidden,\n",
    "                out\n",
    "            )]\n",
    "            if i < depth - 1:\n",
    "                if batch_norm:\n",
    "                    net += [nn.BatchNorm1d(out)]\n",
    "                net += [nn.ReLU()]\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = torch.cat([x, t] + [torch.cos(t ** i) for i in range(self.time_fts)], dim=-1)\n",
    "        return torch.softmax(self.net(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dirichlet_3d(points):\n",
    "    v_a = torch.Tensor([[0, 1.0]])\n",
    "    v_b = torch.Tensor([[-0.5, 0]])\n",
    "    v_c = torch.Tensor([[0.5, 0]])\n",
    "    points = points[:, 0].unsqueeze(-1) * v_a + points[:, 1].unsqueeze(-1) * v_b + points[:, 2].unsqueeze(-1) * v_c\n",
    "    plt.scatter(points[:, 0], points[:, 1])\n",
    "    plt.show()\n",
    "\n",
    "def plot(points, points_b):\n",
    "\n",
    "    v_a = torch.Tensor([[0, 1.0]])\n",
    "    v_b = torch.Tensor([[-0.5, 0]])\n",
    "    v_c = torch.Tensor([[0.5, 0]])\n",
    "    points = points[:, 0].unsqueeze(-1) * v_a + points[:, 1].unsqueeze(-1) * v_b + points[:, 2].unsqueeze(-1) * v_c\n",
    "    plt.scatter(points[:, 0], points[:, 1])\n",
    "\n",
    "\n",
    "    v_a = torch.Tensor([[0, 1.0]])\n",
    "    v_b = torch.Tensor([[-0.5, 0]])\n",
    "    v_c = torch.Tensor([[0.5, 0]])\n",
    "    points_b = points_b[:, 0].unsqueeze(-1) * v_a + points_b[:, 1].unsqueeze(-1) * v_b + points_b[:, 2].unsqueeze(-1) * v_c\n",
    "    plt.scatter(points_b[:, 0], points_b[:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_dirichlet(points: int, alpha):\n",
    "    ret = []\n",
    "    for _ in range(points):\n",
    "        p = np.random.dirichlet(alpha)\n",
    "        ret += [torch.Tensor(p)]\n",
    "    return torch.stack(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dirichlet_mixture(points: int, *alphas):\n",
    "    ret = []\n",
    "    import random\n",
    "    for _ in range(points):\n",
    "        dist = random.choice(alphas)\n",
    "        ret += [torch.Tensor(np.random.dirichlet(dist))]\n",
    "    return torch.stack(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirichlet_3d(generate_simple_dirichlet(1000, [50, 1, 1]))\n",
    "plot_dirichlet_3d(generate_simple_dirichlet(1000, [1, 50, 1]))\n",
    "plot_dirichlet_3d(generate_simple_dirichlet(1000, [1, 1, 50]))\n",
    "plot_dirichlet_3d(generate_dirichlet_mixture(1000, [50, 1, 1], [1, 50, 1], [1, 1, 50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = generate_dirichlet_mixture(10000, [50, 1, 1], [1, 50, 1], [1, 1, 50])\n",
    "dataset = TensorDataset(raw_dataset)\n",
    "test_dataset = TensorDataset(generate_dirichlet_mixture(1000, [50, 1, 1], [1, 50, 1], [1, 1, 50]))\n",
    "train_loader = DataLoader(dataset, 128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, 128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, epochs: int, lr: float = 1e-3, time_eps: float = 0.0):\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    per_epoch = []\n",
    "    per_epoch_test = []\n",
    "    m = NSimplex()\n",
    "    model = model.to(device)\n",
    "    w1s = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        if epoch % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                points = test_dataset.tensors[0].shape[0]\n",
    "                final_traj = m.tangent_euler(torch.stack([torch.Tensor(np.random.dirichlet([1, 1, 1])) for _ in range(points)]), model, 100)\n",
    "                test = torch.cat(test_dataset.tensors)\n",
    "                w1 = m.wasserstein_dist(test, final_traj, power=2)\n",
    "                w1s.append(w1)\n",
    "            print(f\"W1 distance: {w1:.5f}\")\n",
    "        for x_1 in train_loader:\n",
    "            x_1 = x_1[0]\n",
    "            optimizer.zero_grad()\n",
    "            times = torch.rand((x_1.size(0), 1)) * (1.0 - time_eps) + time_eps\n",
    "\n",
    "            # Mapping uniform Dirichlet to our target distribution\n",
    "            x_0 = torch.stack([torch.Tensor(np.random.dirichlet([1, 1, 1])) for _ in range(x_1.size(0))])\n",
    "            x_t = m.geodesic_interpolant(x_0, x_1, times)\n",
    "            target = m.log_map(x_0, x_1)\n",
    "            target = m.parallel_transport(x_0, x_t, target)\n",
    "            out = model(x_t, times)\n",
    "            diff = out - target\n",
    "            loss = m.square_norm_at(x_t, diff).mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += [loss.item()]\n",
    "        print(f\"--- Epoch {epoch+1:03d}/{epochs:03d}: mean loss {np.mean(losses):.5f}\")\n",
    "        per_epoch += [np.mean(losses)]\n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        with torch.no_grad():\n",
    "            for x_1 in test_loader:\n",
    "                x_1 = x_1[0].to(device)\n",
    "                times = torch.rand((x_1.size(0), 1)) * (1.0 - time_eps) + time_eps\n",
    "\n",
    "                # Mapping uniform Dirichlet to our target distribution\n",
    "                x_0 = torch.stack([torch.Tensor(np.random.dirichlet([1, 1, 1])) for _ in range(x_1.size(0))])\n",
    "                x_t = m.geodesic_interpolant(x_0, x_1, times)\n",
    "                target = m.log_map(x_0, x_1)\n",
    "                target = m.parallel_transport(x_0, x_t, target)\n",
    "                out = model(x_t, times)\n",
    "                diff = out - target\n",
    "                loss = m.square_norm_at(x_t, diff).mean()\n",
    "\n",
    "                test_loss += [loss.item()]\n",
    "        print(f\"Test loss {np.mean(test_loss):.5f}\")\n",
    "        per_epoch_test += [np.mean(test_loss)]\n",
    "        \n",
    "    plt.plot(per_epoch, label=\"Train\")\n",
    "    plt.plot(per_epoch_test, label=\"Test\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    plt.plot(w1s)\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "# model = train(MLP(3, 4, 64, False), 50)\n",
    "model = train(MyMLP(input_dim=3, out_dim=2, add_t_emb=True), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import tri\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def euler(model, x_0, steps: int = 1000):\n",
    "    # x_0 batched\n",
    "    delta = 1.0 / steps\n",
    "    x = x_0\n",
    "    for i in range(steps):\n",
    "        t = torch.ones((x_0.size(0), 1)) * delta * i\n",
    "        x = x + delta * model(x, t)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_points(points):\n",
    "    v_a = torch.Tensor([[0, 1.0]])\n",
    "    v_b = torch.Tensor([[-0.5, 0]])\n",
    "    v_c = torch.Tensor([[0.5, 0]])\n",
    "    points = points[:, 0].unsqueeze(-1) * v_a + points[:, 1].unsqueeze(-1) * v_b + points[:, 2].unsqueeze(-1) * v_c\n",
    "    return points\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def tangent_euler(model, x_0, steps: int = 100, evol: bool = False):\n",
    "    dt = 1.0 / steps\n",
    "    x = x_0\n",
    "    every = 100\n",
    "    m = NSimplex()\n",
    "    xs = []\n",
    "    for i in range(steps):\n",
    "        t = torch.ones((x.size(0), 1)) * dt * (i + 1)\n",
    "        x = m.exp_map(x, model(x, t) * dt)\n",
    "        if (i + 1) % every == 0:\n",
    "            xs += [x]\n",
    "    if evol:\n",
    "        f, axs = plt.subplots(nrows=(len(xs) // 4) + 1, ncols=4, figsize=(15, 15))\n",
    "        for i, x in enumerate(xs):\n",
    "            pts = get_points(x)\n",
    "            axs[i // 4, i % 4].scatter(pts[:, 0], pts[:, 1], label=f\"t={(i + 1) * every}\")\n",
    "        plt.show()\n",
    "    return x\n",
    "\n",
    "\n",
    "class Dirichlet(object):\n",
    "    def __init__(self, alpha):\n",
    "        from math import gamma\n",
    "        from operator import mul\n",
    "        self._alpha = np.array(alpha)\n",
    "        self._coef = gamma(np.sum(self._alpha)) / \\\n",
    "                           np.multiply.reduce([gamma(a) for a in self._alpha])\n",
    "    def pdf(self, x):\n",
    "        '''Returns pdf value for `x`.'''\n",
    "        from operator import mul\n",
    "        return self._coef * np.multiply.reduce([xx ** (aa - 1)\n",
    "                                               for (xx, aa)in zip(x, self._alpha)])\n",
    "\n",
    "\n",
    "def viz_model(model):\n",
    "    import dirichlet\n",
    "    points = torch.stack([torch.Tensor(np.random.dirichlet([1, 1, 1])) for _ in range(1000)])\n",
    "    dest = tangent_euler(model, points)\n",
    "    corners = np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]])\n",
    "    triangle = tri.Triangulation(corners[:, 0], corners[:, 1])\n",
    "    refiner = tri.UniformTriRefiner(triangle)\n",
    "    trimesh = refiner.refine_triangulation(subdiv=2)\n",
    "    AREA = 0.5 * 1 * 0.75**0.5\n",
    "    pairs = [corners[np.roll(range(3), -i)[1:]] for i in range(4)]\n",
    "    def xy2bc(xy, tol=1.e-4):\n",
    "        '''Converts 2D Cartesian coordinates to barycentric.'''\n",
    "        tri_area = lambda xy, pair: 0.5 * np.linalg.norm(np.cross(*(pair - xy)))\n",
    "        coords = np.array([tri_area(xy, p) for p in pairs]) / AREA\n",
    "        return np.clip(coords, tol, 1.0 - tol)\n",
    "\n",
    "    class Dirichlet(object):\n",
    "        def __init__(self, alpha):\n",
    "            from math import gamma\n",
    "            from operator import mul\n",
    "            self._alpha = np.array(alpha)\n",
    "            self._coef = gamma(np.sum(self._alpha)) / np.multiply.reduce([gamma(a) for a in self._alpha])\n",
    "        def pdf(self, x):\n",
    "            '''Returns pdf value for `x`.'''\n",
    "            from operator import mul\n",
    "            return self._coef * np.multiply.reduce([xx ** (aa - 1.0)\n",
    "                                                for (xx, aa) in zip(x, self._alpha)])\n",
    "    alphas = dirichlet.mle(dest.numpy(), tol=1e-5)\n",
    "    print(alphas)\n",
    "    pdf = Dirichlet(alphas)\n",
    "    pvals = [pdf.pdf(xy2bc(x)) for x in zip(trimesh.x, trimesh.y)]\n",
    "\n",
    "    plt.tricontourf(trimesh, pvals, 100, cmap=\"jet\")\n",
    "    plt.show()\n",
    "    plot(points, dest)\n",
    "    plot(raw_dataset, dest)\n",
    "\n",
    "\n",
    "viz_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
